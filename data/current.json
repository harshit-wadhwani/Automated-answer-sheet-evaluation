{"data": [{"date": "2024-05-11", "code": "18AI88", "score": "5", "question": "What is Machine Learning? Give 3 examples where ML can be used.", "answer": "Machine Learning is the] field of study that gives computers the ability to learn without being explicitly programmed. OR A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E.  3 examples are as follows: Detecting tumors in brain scans This is semantic segmentation, where each pixel in the image is classified (as we want to determine the exact location and shape of tumors), typically using CNNs as well  Creating a chatbot or a personal assistant This involves many NLP components, including natural language understanding (NLU) and question-answering modules. Summarizing long documents automatically This is a branch of NLP called text summarization, again using the same tools.", "Qid": "18AI881", "university": "Visvesvaraya Technological University", "subject": "Machine Learning", "time": "120 "}, {"date": "2024-05-11", "code": "18AI88", "score": "10", "question": "What are the main challenges in ML?", "answer": "Insufficient Quantity of Training Data: It takes a lot of data for most Machine Learning algorithms to work properly. Even for very simple problems you typically need thousands of examples, and for complex problems such as image or speech recognition you may need millions of examples (unless you can reuse parts of an existing model).  Nonrepresentative Training Data In order to generalize well, it is crucial that your training data be representative of the new cases you want to generalize to. This is true whether you use instance-based learning or model-based learning. It is crucial to use a training set that is representative of the cases you want to generalize to. This is often harder than it sounds: if the sample is too small, you will have sampling , but even very large samples can be nonrepresentative if the sampling method is flawed. This is called sampling bias.  Poor-Quality Data Obviously, if your training data is full of errors, outliers, and noise (e.g., due to poor\u0002quality measurements), it will make it harder for the system to detect the underlying patterns, so your system is less likely to perform well. It is often well worth the effort to spend time cleaning up your training data. The truth is, most data scientists spend a significant part of their time doing just that. The following are a couple of examples of when you\u2019d want to clean up training data: * If some instances are clearly outliers, it may help to simply discard them or try to fix the errors manually. * If some instances are missing a few features ,you must decide whether you want to ignore this attribute altogether, ignore these instances, fill in the missing values (e.g., with the median age), or train one model with the feature and one model without it.  Irrelevant Features Your system will only be capable of learning if the training data contains enough relevant features and not too many irrelevant ones. A critical part of the success of a Machine Learning project is coming up with a good set of features to train on. This process, called feature engineering, involves the following steps: * Feature selection (selecting the most useful features to train on among existing features) * Feature Extraction * Creating new features by gathering new data  Overfitting the Training Data Overgeneralizing is something that we humans do all too often, and unfortunately machines can fall into the same trap if we are not careful. In Machine Learning this is called overfitting: it means that the model performs well on the training data, but it does not generalize well. Overfitting happens when the model is too complex relative to the amount and noisiness of the training data. Here are possible solutions: * Simplify the model by selecting one with fewer parameters (e.g., a linear model rather than a high-degree polynomial model), by reducing the number of attributes in the training data, or by constraining the model. * Gather more training data. * Reduce the noise in the training data (e.g., fix data errors and remove outliers)  Underfitting the Training Data As you might guess, underfitting is the opposite of overfitting: it occurs when your model is too simple to learn the underlying structure of the data. For example, a linear model of life satisfaction is prone to underfit; reality is just more complex than the model, so its predictions are bound to be inaccurate, even on the training examples. Here are the main options for fixing this problem: * Select a more powerful model, with more parameters. * Feed better features to the learning algorithm (feature engineering). * Reduce the constraints on the model (e.g., reduce the regularization hyperparameter).", "Qid": "18AI882", "university": "Visvesvaraya Technological University", "subject": "Machine Learning", "time": "120 "}, {"date": "2024-05-11", "code": "18AI88", "score": "5", "question": "Explain voting classifier, Bagging and Pasting, Out-of-Bag Evaluation.", "answer": "Voting classifier: Suppose you have trained a few classifiers, each one achieving about 80% accuracy. You may have a Logistic Regression classifier, an SVM classifier, a Random Forest classifier, a K-Nearest Neighbors classifier, and perhaps a few more. A very simple way to create an even better classifier is to aggregate the predictions of each classifier and predict the class that gets the most votes. This majority-vote classifier is called a hard voting classifier. Somewhat surprisingly, this voting classifier often achieves a higher accuracy than the best classifier in the ensemble. In fact, even if each classifier is a weak learner, the ensemble can still be a strong learner (achieving high accuracy), provided there are a sufficient number of weak learners and they are sufficiently diverse. One way to get a diverse set of classifiers is to use very different training algorithms, as just discussed. Another approach is to use the same training algorithm for every predictor and train them on different random subsets of the training set. When sampling is performed with replacement, this method is called bagging1(short for bootstrap aggregating2). When sampling is performed without replacement, it is called pasting. In other words, both bagging and pasting allow training instances to be sampled several times across multiple predictors, but only bagging allows training instances to be sampled several times for the same predictor.  Once all predictors are trained, the ensemble can make a prediction for a new instance by simply aggregating the predictions of all predictors. The aggregation function is typically the statistical mode (i.e., the most frequent prediction, just like a hard voting classifier) for classification, or the average for regression. Each individual predictor has a higher bias than if it were trained on the original training set, but aggregation reduces both bias and variance. Generally, the net result is that the ensemble has a similar bias but a lower variance than a single predictor trained on the original training set. Out of Box Evaluation : With bagging, some instances may be sampled several times for any given predictor, while others may not be sampled at all. By default a BaggingClassifier samples m training instances with replacement (bootstrap=True), where m is the size of the training set. Since a predictor never sees the oob instances during training, it can be evaluated on these instances, without the need for a separate validation set. You can evaluate the ensemble itself by averaging out the oob evaluations of each predict.", "Qid": "18AI883", "university": "Visvesvaraya Technological University", "subject": "Machine Learning", "time": "120 "}, {"date": "2024-05-11", "code": "18AI88", "score": "3", "question": "List the 3 major types of Machine Learning Algorithm.", "answer": "Supervised Learning: These systems are trained with labeled data, where each input is associated with a corresponding target output. During training, the model learns the mapping between inputs and outputs, enabling it to make predictions or decisions when new data is presented. Unsupervised Learning: Unsupervised learning algorithms deal with unlabeled data. The model attempts to find patterns or structures in the data without explicit guidance. Common tasks include clustering similar data points together or reducing the dimensionality of the data. Reinforcement Learning: In reinforcement learning, an agent learns to interact with an environment by taking actions and receiving feedback in the form of rewards or penalties. The goal is to learn the optimal strategy or policy that maximizes cumulative reward over time.", "Qid": "18AI884", "university": "Visvesvaraya Technological University", "subject": "Machine Learning", "time": "120 "}, {"date": "2024-05-11", "code": "18AI88", "score": "2", "question": "Differentiate Supervised and Unsupervised Learning with the help of an example.", "answer": "Supervised learning example includes classifying email as a spam or a non spam email whereas unsupervised learning example includes using customer purchase history data to group them into various segments.", "Qid": "18AI885", "university": "Visvesvaraya Technological University", "subject": "Machine Learning", "time": "120 "}]}